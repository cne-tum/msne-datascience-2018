{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized linear models\n",
    "## Marcel Nonnenmacher, Jan-Mathis LÃ¼ckmann, Pedro Goncalves, Jakob Macke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One of the central problems in systems neuroscience is that of characterizing the functional relationship between sensory stimuli and neural spike responses. Investigators call this the neural coding problem, because the spike trains of neurons can be considered a code by which the brain represents information about the state of the external world. One approach to understanding this code is to build mathematical models of the mapping between stimuli and spike responses; the code can then be interpreted by using the model to predict the neural response to a stimulus, or to decode the stimulus that gave rise to a particular response. [(Pillow, 2007)](http://pillowlab.princeton.edu/pubs/Pillow_BBchap07.pdf)\n",
    "\n",
    "Here, we will build probabilistic models for the response of a single neuron, starting from a simple model, that we will then extend. Conditional on a stimulus $x$ and model parameters $\\theta$ we will model the probability of a neural reponse $y$, i.e., $p(y|x, \\theta)$. Our central goal will be to find model parameters $\\theta$ such that the $p(y|x,\\theta)$ is a good fit to a dataset of stimulus-response pairs we observed, $\\mathcal{D} = \\{ (x_k,y_k) \\}_{k=1}^K$. \n",
    "\n",
    "\n",
    "### Goals of these exercises\n",
    "\n",
    "Central to inferring the best fitting parameters will be the likelihood function of the model. The simplest method goes by simply maximizing the likelihood using its gradient with respect to the model parameters (a technique called maximum likelihood estimation, MLE). You will learn to incorporate prior knowledge on the parameters, which leads to a method called maximum a posteriori (MAP). Finally, you will learn automatic differentiation (AD) which --- as the name suggests --- provides a automatic way to calcuate gradients of an objective function (here: the likelihood of parameters given the data). AD is a central ingredient to machine learning methods that are becoming increasingly popular.\n",
    "\n",
    "### Assumptions and notation\n",
    "\n",
    "Throughout this tutorial, we will adopt the following conventions:\n",
    "\n",
    "- $T$ is the number of time bins within one trial; $t$ always specifies a time bin;\n",
    "- $K$ is the number of trials in the experiment; $k$ always identifies a trial;\n",
    "- to make the notation lighter, we will sometimes drop the subscript $k$;\n",
    "- $\\hat{\\pi}(\\cdot)$ indicates an unnormalized probability, and $\\pi(\\cdot)$ the same probability normalize to integrate to 1;\n",
    "- $\\mathcal{L}(\\boldsymbol{\\theta}) = p(\\mathbf{y}\\, |\\, \\boldsymbol{\\theta})$ is the likelihood of the vector of parameters $\\boldsymbol{\\theta}$ for the (fixed) data $\\mathbf{y}$.\n",
    "\n",
    "For all models we consider, we assume that time is discretized in bins of size $\\Delta$. Given $z_t$, the instantaneous *input rate* of a neuron at time $\\Delta \\cdot t$, the spike counts $y_t$ are assumed to be independent, and distributed according to\n",
    "\n",
    "$\\begin{equation}\n",
    "    y_t \\sim \\mathrm{Poisson}\\big(\\eta(z_t)\\big)\n",
    "\\end{equation}$\n",
    "\n",
    "where  $\\eta(\\cdot)$ is corresponding canonical link function (here, we will always use $\\eta(\\cdot) = \\exp(\\cdot)$ for Poisson). We further assume that there is a linear dependence between $z_i$ and a set of external covariates $\\mathbf{x}_t$ at time $t$, i.e. $z_t = \\boldsymbol{\\theta}^\\top \\mathbf{x}$, and $\\boldsymbol{\\theta}$ is a vector of parameters which fully characterizes the neuron.\n",
    "\n",
    "Experiments are composed of $K$ trials, each subdivided into $T$ bins. \n",
    "\n",
    "Note, and in contrast to the lectures, we assume that the rate $\\mu_t$ is already 'per bin size', i.e. the expected number of spikes is $\\mu$ (and not $\\mu\\Delta$, as we had in lectures).\n",
    "\n",
    "For a Poisson neuron, the probability of producing $n$ spikes in an interval of size $\\Delta$ is given by \n",
    "\n",
    "$\\begin{align}\n",
    "P(y_t=n| \\mu)= \\frac{\\mu^n e^{-\\mu} }{n!}\n",
    "\\end{align}$\n",
    "\n",
    "\n",
    "## Exercise 1 (from lectures)\n",
    "\n",
    "Assume that you have spike counts $n_1$ to $n_K$ from $K$ trials, calculate the maximum likelihood estimate (MLE) of $\\mu$.\n",
    "\n",
    "\n",
    "## LNP model\n",
    "\n",
    "The stimulus $\\mathbf{u}_t$ is a white noise sequence, and the input rate is:\n",
    "\n",
    "$\\begin{equation}\n",
    "    z_t = \\mathbf{\\beta}^\\top \\mathbf{u}_{t-\\delta{}+1:t} + b = \\boldsymbol{\\theta}^\\top \\mathbf{x}\n",
    "\\end{equation}$,\n",
    "\n",
    "i.e. $z_t$ is the result of a linear filter $\\beta$ applied to the recent stimulus history $\\mathbf{u}_{t-\\delta{}+1:t}$, plus some offset $b$. This results in a vector of covariates at time $t$\n",
    "$\\mathbf{x}_{kt} = \\big[1, \\mathbf{u}_{kt-\\delta{}+1},\\ldots, \\mathbf{u}_{kt} \\big]^\\top$ for temporal filter length $\\delta \\in \\mathbb{N}$. Note that we can deal with any form of input in the second and third column of $\\mathbf{x}_{kt}$, not just white noise.\n",
    "\n",
    "The vector of parameters is $\\boldsymbol{\\theta} = \\left[b, \\beta^\\top \\right]^\\top$.\n",
    "\n",
    "\n",
    "\n",
    "### Simulating data from the model\n",
    "\n",
    "Next, we  will want to generate data using this model. Execute the following code cell, which will load some functions you will need throughout the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%run -i helpers.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell generates a matrix $\\mathbf{x}$ as specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize = 0.001  # seconds\n",
    "T = 10000        # 1s trials\n",
    "K = 10  # number of trials\n",
    "nbins = T*K\n",
    "\n",
    "delta = 10       # length of temporal filter\n",
    "\n",
    "# stimulus\n",
    "U = np.random.normal(size=nbins)\n",
    "\n",
    "def toyDesignMatrix(U=U, T=T, K=K):    \n",
    "    nbins = T*K\n",
    "    X = np.zeros((delta+1, nbins))\n",
    "\n",
    "    X[0,:] = 1.       # bias\n",
    "    if delta > 0:\n",
    "        X[1, :] = U   # instantaneous input\n",
    "    for i in range(1,delta):\n",
    "        X[i+1, i+1:] = U[:-(i+1)]\n",
    "\n",
    "    return X\n",
    "\n",
    "X = toyDesignMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define $\\mathbf{\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground-truth vector of parameters\n",
    "b = -6  # controls the offset and hence overall firing rate\n",
    "beta = np.cos( np.linspace(0, PI, delta))\n",
    "theta = np.hstack([b, beta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given `X` and `theta`, we want to generate sample spike trains. In the following cell, we do so by just using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "for k in range(10):\n",
    "    y, fr = toyModel(X, theta)  # spike train, firing rate\n",
    "    ys.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... plotting spike rasters and PSTH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd1b944ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(311)\n",
    "plt.plot(U[:200])\n",
    "plt.subplot(312)\n",
    "plt.imshow(np.asarray(ys)[:,:200], aspect=4, interpolation='None');\n",
    "plt.subplot(313)\n",
    "plt.plot(np.asarray(ys)[:, :200].mean(axis=0) / binsize);  # PSTH\n",
    "plt.plot(fr[:200], linewidth=2);  # firing rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Optional: Try implementing the model yourself\n",
    "\n",
    "(Optional): Above, you used an implementation of the model we provided. You can try implementing the model as stated above yourself. To do so, complete the following function template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toyModelExercise(X, theta):\n",
    "    # TODO: given stimulus and theta, return spikes and firing rate\n",
    "    return y, fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether this model is correct, reproduce the PSTHs for both models and compare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### MLE inference\n",
    "\n",
    "#### Likelihood ####\n",
    "\n",
    "The likelihood defines a model that connects model parameters to the observed data: \n",
    "\n",
    "$\\begin{align}\n",
    "\\log \\mathcal{L}(\\boldsymbol{\\theta}) &= \\log p(\\mathbf{y} | \\boldsymbol{\\theta}) = \\log \\bigg[ \\prod_{k=1}^K \\prod_{t=1}^T p(y_{kt} | b, \\beta_1, \\beta_2) \\bigg] \\\\ &= \\sum_{k=1}^K \\sum_{t=1}^T \\log p(y_{tk} | b, \\beta_1, \\beta_2) = \\sum_{k=1}^K \\sum_{t=1}^T \\big[ z_{tk} y_{tk} - \\mathrm{e}^{z_{tk}} \\big], \n",
    "\\end{align}$\n",
    "\n",
    "where as above $z_{tk} = \\theta^\\top \\mathbf{x}_{tk} = b + \\beta^\\top \\mathbf{u}_{tk-\\delta{}+1:tk}$.\n",
    "\n",
    "Large $\\mathcal{L}$ for a given set of parameters $\\theta$ indicates that the data is likely under that parameter set. We can iteratively find more likely parameters, starting from some initial guess $\\theta_0$, by gradient ascent on $\\mathcal{L}$.\n",
    "\n",
    "For this model, the likelihood function has a unique maximum.\n",
    "\n",
    "\n",
    "#### Gradients ####\n",
    "\n",
    "**Exercise 1:** Using pen and paper, derive the gradient of the $\\log \\mathcal{L}$ with respect to $\\mathbf{\\theta}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE parameter inference\n",
    "\n",
    "We will now want to the use gradient you just derived to do parameter inference. For that, we will need to implement the functions `ll` and `dll` (the log-likelihood function and its derivative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1: ** Implement `ll` and `dll` in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# say, we got a single spike train\n",
    "y, fr = toyModel(X, theta)  # spike train, firing rate\n",
    "\n",
    "def ll(theta):\n",
    "    # TODO: implement log-likelihood function\n",
    "    return NotImplemented\n",
    "\n",
    "def dll(theta):\n",
    "    # TODO: implement derivative of log-likelihood function wrt theta\n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.2**: Assume the true parameters that we used to generate the data, $\\mathbf{\\theta}^*$, were unknown. We want to recover $\\mathbf{\\theta}^*$ starting from an initial guess $\\mathbf{\\theta}_0$. Fill the gaps in the code block below. How good do you recover $\\mathbf{\\theta}^*$? What happens if you change `step_size`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_star : [-6.          1.          0.93969262  0.76604444  0.5         0.17364818\n",
      " -0.17364818 -0.5        -0.76604444 -0.93969262 -1.        ]\n",
      "theta_0 : [-0.86170838 -0.52051442  1.04976038 -0.58227439  1.09268153  0.7375664\n",
      " -0.94986715  0.0133238  -0.62592408 -1.86035052 -0.09161797]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ed2b5c1312db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtheta_hat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtheta_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradientAscent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta_initial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'theta_hat : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-ed2b5c1312db>\u001b[0m in \u001b[0;36mgradientAscent\u001b[1;34m(theta_initial, step_size, num_iterations)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mlog_likelihood\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtheta_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta_hat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtheta_hat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'ellipsis'"
     ]
    }
   ],
   "source": [
    "theta_true = theta.copy()\n",
    "theta_initial = np.random.randn(len(theta))\n",
    "\n",
    "print('theta_star : {}'.format(theta_true))\n",
    "print('theta_0 : {}'.format(theta_initial))\n",
    "\n",
    "def gradientAscent(theta_initial, step_size=0.0001, num_iterations=1000):\n",
    "    theta_hat = theta_initial.copy()\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        # TODO: fix the next lines\n",
    "        log_likelihood = ...\n",
    "        gradient = ... \n",
    "        theta_hat = theta_hat + ...\n",
    "\n",
    "    return theta_hat\n",
    "\n",
    "theta_hat = gradientAscent(theta_initial)\n",
    "print('theta_hat : {}'.format(theta_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the model\n",
    "\n",
    "Our simple model assumed independent firing in each time-bin that only depends on the stimulus. In reality, we know that the activity of neurons depends also on their recent firing history. \n",
    "\n",
    "The GLM frameworks allows to flexibly extend our model simply by adding additional covariates and corresponding parameters, i.e. by adding columns to design matrix $\\mathbf{X}$ and entries to parameter vector $\\theta$. \n",
    "\n",
    "Let us try introducing the recent spiking history $\\mathbf{y}_{kt-\\tau}, \\ldots, \\mathbf{y}_{kt-1}$ as additional covariates.\n",
    "\n",
    "The vector of covariates at time $t$ becomes\n",
    "$\\mathbf{x}_{kt} = \\big[1, \\mathbf{u}_{kt-\\delta+1 \\ : \\ tk}, \\mathbf{y}_{kt-\\tau \\ : \\ tk-1}\\big]^\\top$, \n",
    "\n",
    "and we extend the vector of parameters as $\\boldsymbol{\\theta} = \\left[b, \\mathbf{\\beta}^\\top, \\mathbf{\\psi}^\\top \\right]^\\top$, with history kernel $\\mathbf{\\psi} \\in \\mathbb{R}^\\tau$ and history kernel length $\\tau \\in \\mathbb{N}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What other covariates could help improve our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE Inference\n",
    "\n",
    "**Exercise 3.1:** Write a function that implements the new design matrix $\\mathbf{X}$ (now depends on data $\\mathbf{y}$). Note that we provide a function `createDataset()` to generate data from the extended model with given parameter vector $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tau = 5  # length of history kernel (in bins)\n",
    "\n",
    "psi = - 1.0 * np.arange(0, tau)[::-1]\n",
    "theta_true = np.hstack((theta, psi))\n",
    "y = createDataset(U, T, K, theta_true, delta)\n",
    "\n",
    "def extendedDesignMatrix(y): \n",
    "    # TODO: implement design matrix X with \n",
    "    # X[kt,:] = [1, w*cos(t), w*sin(t), y_{kt-tau:kt-1}]\n",
    "    return NotImplemented\n",
    "\n",
    "X = extendedDesignMatrix(y) # you might have to re-run the cell defining ll() and dll() \n",
    "                             # to update the used design matrix X and data y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2:** Write down the gradients for the extended model. What changes from our earlier simpler model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP inference\n",
    "\n",
    "The solution $\\hat{\\theta}$ obtained by gradient ascent on the log-likelihood depends on the data $\\mathcal{D} = \\{ (x_{tk}, y_{tk}) \\}_{(t,k)}$. In particular for very short traces and few trials, this data only weakly constrains the solution. \n",
    "We can often improve our obtained solutions by adding prior knowledge regarding what 'good' solutions should look like. In probabilistic modeling, this can be done by introducing prior distributions $p(\\theta)$ on the model parameters, which together with the likelihood $\\mathcal{L}$ define a posterior distribution over parameters given the data $p(\\theta | \\mathbf{y})$, \n",
    "\n",
    "$$ \\log p(\\theta | \\mathbf{y}) = \\log p(\\mathbf{y}|\\theta) + \\log p(\\theta) - \\log p(\\mathbf{y}) = \\mathcal{L}(\\theta) + \\log p(\\theta) + const.$$\n",
    "\n",
    "Maximum a-posterio (MAP) estimates parameters $\\theta$ by gradient ascent on the (log-)posterior. \n",
    "\n",
    "We will assume zero-mean Gaussian priors on $\\beta, \\psi$, i.e. \n",
    "\\begin{align}\n",
    "p(\\beta) &= \\mathcal{N}(0, \\Sigma_\\beta) \\\\\n",
    "p(\\psi) &= \\mathcal{N}(0, \\Sigma_\\psi).\n",
    "\\end{align}\n",
    "We will not assume an explicit prior on $b$, which effectively assumes $b$ to be distributed 'uniformly' over $\\mathbb{R}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "\n",
    "Compared to maximum likelihood, MAP only requires adding the prior gradient:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta} p(\\theta|\\mathbf{y}) =  \\frac{\\partial}{\\partial \\theta} \\mathcal{L}(\\theta) + \\frac{\\partial}{\\partial \\theta}p(\\theta)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4: ** Derive the gradients for the prior. If you get stuck, or if you want to verify the solution, ask the tutors for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** Fill gaps in codeblock below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## priors\n",
    "\n",
    "# select prior covariance for input weights\n",
    "Sig_beta = np.eye(delta)                     \n",
    "\n",
    "# select prior covariance for history kernel\n",
    "ir = np.atleast_2d(np.arange(tau, 0, -1))\n",
    "Sig_psi = np.exp(- np.abs(ir.T - ir)/5)  # assuming smoothness\n",
    "\n",
    "# convenience\n",
    "P_beta, P_psi = np.linalg.inv(Sig_beta), np.linalg.inv(Sig_psi)\n",
    "\n",
    "\n",
    "## functions and gradients\n",
    "\n",
    "def po(theta):\n",
    "    # TODO: implement log-posterior density function\n",
    "    return NotImplemented\n",
    "\n",
    "def dpo(theta):\n",
    "    # TODO: implement derivative of log-posterior density function wrt theta\n",
    "    return NotImplemented\n",
    "\n",
    "# Hint: it can be helpful to first derive the functions for the prior below: \n",
    "\n",
    "def pr(theta):\n",
    "    # TODO: implement log-prior density function\n",
    "    return NotImplemented\n",
    "\n",
    "def dpr(theta):\n",
    "    # TODO: implement derivative of log-prior density function wrt theta\n",
    "    return NotImplemented\n",
    "\n",
    "# leave as is\n",
    "def ll(theta):\n",
    "    z = np.dot(theta, X)\n",
    "    return np.sum( y * z - link(z) )\n",
    "\n",
    "# leave as is\n",
    "def dll(theta):\n",
    "    z = np.dot(theta, X)\n",
    "    r = y - link(z)\n",
    "    return np.dot(X, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Numerical gradient checking -- use the code below to numerically ensure your gradients are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "thrn = np.random.normal(size=theta_true.shape)\n",
    "print(optimize.check_grad(ll, dll, thrn))\n",
    "print(optimize.check_grad(pr, dpr, thrn))\n",
    "print(optimize.check_grad(po, dpo, thrn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Do inference (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = createDataset(1000, 1, theta_true, omega)\n",
    "\n",
    "# TODO: implement gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation \n",
    "\n",
    "Instead of calculating the gradients w.r.t. the model parameters by hand, we can calculate them automatically. Our objective function consists of many elementary functions, each of which is differentiable. [Automatic differentiation (AD)](https://en.wikipedia.org/wiki/Automatic_differentiation) applies the chain rule to the expression graph of our objective to find the gradient. \n",
    "\n",
    "Here, we will use a Python library called `autograd` to find the gradient of our objective. AD is a central ingredient in libraries used for training artifical neural networks, including theano, TensorFlow and PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Install the [`autograd` package](https://github.com/HIPS/autograd) through your package manager. \n",
    "\n",
    "Depending on how things are set up on your machine, install `autograd` by `pip3 install autograd --user` or by `pip install autograd --user`. \n",
    "\n",
    "You might need to restart the notebook kernel in case the simple example which follows fails with an import error. If you restart the kernel, make sure to re-run the cells. You can do that by choosing `Kernel > Restart & Run All` from the menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autograd` by a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np   # thinly-wrapped numpy\n",
    "from autograd import grad     # the only autograd function you may ever need\n",
    "\n",
    "def tanh(x):                  # Define a function\n",
    "    y = np.exp(-x)\n",
    "    return (1.0 - y) / (1.0 + y)\n",
    "\n",
    "grad_tanh = grad(tanh)        # Obtain its gradient function\n",
    "\n",
    "print('Gradient at x=1.0 (autograd) : {}'.format(grad_tanh(1.0)))\n",
    "\n",
    "print('Gradient at x=1.0 (finite diff): {}'.format((tanh(1.0001) - tanh(0.9999)) / 0.0002))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-f42dcd6cf475>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-f42dcd6cf475>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ipython nbconvert exercises.ipynb --to pdf\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ipython nbconvert exercises.ipynb --to pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8**: Redo exercise 2 using `autograd`. To do so, go to the first code cell of the notebook. Instead of `import numpy as np` use `import autograd.numpy as np`. Restart the notebook kernel and run through the notebook from the beginning, till you get to the point where you implemented `dll`. Replace `dll` by a AD version that relies on the function `grad` (see simple example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:** Similar to the previous exercise, adapt the extended version of the model to rely on `autograd` for differentiation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10 (open ended)** : Come up with your own extensions to one of the models. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
